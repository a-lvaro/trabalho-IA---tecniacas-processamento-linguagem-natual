{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extração do texto do PDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Listagem de palavras frequentes que não alteram o significado do texto (em ingles)\n",
    "nltk.download('stopwords')\n",
    "irrelevantes = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tratamento de flexão de palavras por lematização\n",
    "tk = nltk.tokenize.WhitespaceTokenizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lmt = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#A biblioteca nltk não tem dados para lematização em PT\n",
    "def lematize_text(text):\n",
    "    lem_words = list()\n",
    "    for word in tk.tokenize(text):\n",
    "        lem_words.append(lmt.lemmatize(word))\n",
    "    return lem_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfReader('ArquivosEN/1608.06902.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "textin = reader.pages[0].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n"
     ]
    }
   ],
   "source": [
    "lematizedTXT = lematize_text(textin)\n",
    "print(len(lematizedTXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removendo elementos ruins - função bem custosa, deve ter alguma que faz isso mais facil\n",
    "for word in lematizedTXT:\n",
    "    for stopword in irrelevantes:\n",
    "        if word == stopword:\n",
    "            lematizedTXT.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "['Recurrent', 'Neural', 'Networks', 'With', 'Limited', 'Numerical', 'Precision', 'Joachim', 'Ott\\x03,', 'Zhouhan', 'Linz,Ying', 'Zhangz,', 'Shih-Chii', 'Liu\\x03,', 'Yoshua', 'Bengiozy', '\\x03Institute', 'Neuroinformatics,', 'University', 'Zurich', 'ETH', 'Zurich', 'ottj@ethz.ch,', 'shih@ini.ethz.ch', 'zDépartement', 'd’informatique', 'et', 'de', 'recherche', 'opérationnelle,', 'Université', 'de', 'Montréal', 'yCIFAR', 'Senior', 'Fellow', '{zhouhan.lin,', 'ying.zhang}@umontreal.ca', 'Abstract', 'Recurrent', 'Neural', 'Networks', '(RNNs)', 'produce', 'state-of-art', 'performance', 'many', 'machine', 'learning', 'task', 'demand', 'resource', 'term', 'memory', 'computational', 'power', 'often', 'high.', 'Therefore,', 'great', 'interest', 'optimizing', 'computation', 'performed', 'model', 'especially', 'considering', 'development', 'specialized', 'low-power', 'hardware', 'deep', 'networks.', 'One', 'way', 'reducing', 'computational', 'need', 'limit', 'numerical', 'precision', 'network', 'weight', 'biases.', 'This', 'ha', 'led', 'different', 'proposed', 'rounding', 'method', 'applied', 'far', 'Convolutional', 'Neural', 'Networks', 'Fully-Connected', 'Networks.', 'This', 'paper', 'address', 'question', 'best', 'reduce', 'weight', 'precision', 'training', 'case', 'RNNs.', 'We', 'present', 'result', 'use', 'different', 'stochastic', 'deterministic', 'reduced', 'precision', 'training', 'method', 'applied', 'three', 'major', 'RNN', 'type', 'tested', 'several', 'datasets.', 'The', 'result', 'show', 'weight', 'binarization', 'method', 'work', 'RNNs.', 'However,', 'stochastic', 'deterministic', 'ternarization,', 'pow2-ternarization', 'method', 'gave', 'rise', 'low-precision', 'RNNs', 'produce', 'similar', 'even', 'higher', 'accuracy', 'certain', 'datasets', 'therefore', 'providing', 'path', 'towards', 'training', 'efﬁcient', 'implementation', 'RNNs', 'specialized', 'hardware.', '1', 'Introduction', 'A', 'Recurrent', 'Neural', 'Network', '(RNN)', 'speciﬁc', 'type', 'neural', 'network', 'able', 'process', 'input', 'output', 'sequence', 'variable', 'length.', 'Because', 'nature,', 'RNNs', 'suitable', 'sequence', 'modeling.', 'Various', 'RNN', 'architecture', 'proposed', 'recent', 'years,', 'based', 'different', 'form', 'non-linearity,', 'Gated', 'Recurrent', 'Unit', '(GRU)', '[Cho', 'et', 'al.,', '2014]', 'Long-Short', 'Term', 'Memory', '(LSTM)', '[Hochreiter', 'et', 'al.,', '1997].', 'They', 'enabled', 'new', 'level', 'performance', 'many', 'task', 'speech', 'recognition', '[Amodei', 'et', 'al.,', '2015][Chan', 'et', 'al.,', '2015],', 'machine', 'translation', '[Devlin', 'et', 'al.,', '2014][Chung', 'et', 'al.,', '2016][Sutskever', 'et', 'al.,', '2014],', 'even', 'video', 'game', '[Mnih', 'et', 'al.,', '2015]', 'Go[Silver', 'et', 'al.,', '2016].', 'Compared', 'standard', 'feed-forward', 'networks,', 'RNNs', 'often', 'take', 'longer', 'train', 'demanding', 'memory', 'computational', 'power.', 'For', 'example,', 'take', 'week', 'train', 'model', 'state-of-the-art', 'machine', 'translation', 'speech', 'recognition.', 'Thus', 'vital', 'importance', 'accelerate', 'computation', 'reduce', 'training', 'time', 'networks.', 'On', 'other', 'hand,', 'even', 'run-time,', 'model', 'require', 'much', 'term', 'computational', 'resource', 'want', 'deploy', 'model', 'onto', 'low-power', 'embedded', 'hardware', 'devices.', 'Increasingly,', 'dedicated', 'deep', 'learning', 'hardware', 'platform', 'including', 'FPGAs', '[Farabet', 'et', 'al.,', '2011]', 'custom', 'chip', '[Sim', 'et', 'al.,', '2016]', 'reporting', 'higher', 'computational', 'efﬁciencies', 'tera', 'operation', 'per', 'second', 'per', 'watt', '(TOPS/W).', 'These', 'platform', 'targeted', 'deep', 'CNNs.', 'If', 'low-precision', 'RNNs', 'able', 'report', 'same', 'performance,', 'saving', 'reduction', 'multiplier', '(the', 'circuit', 'take', 'space', 'energy)', 'memory', 'storage', 'weight', 'would', 'even', 'larger', 'bit', 'precision', 'multiplier', 'needed', '2', '3', 'gate', 'gated', 'RNN', 'unit', 'be', 'reduced', 'the', 'multiplier', 'removed', 'completely.', '1arXiv:1608.06902v2', '[cs.NE]', '26', 'Feb', '2017']\n"
     ]
    }
   ],
   "source": [
    "#Deu certo, mas ainda tem palavras que precisariam ser tratadas\n",
    "print(len(lematizedTXT))\n",
    "print(lematizedTXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existem caracteres estranhos (Ott\\x03,)\n",
    "#Verificar pontuação, ela pode não ser tão importante\n",
    "#Existem alguns trechos em espanhol(?)\n",
    "#Palavras compostas como low-precision\n",
    "#1arXiv:1608.06902v2 (?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package floresta to /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/alvaro/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importa dados que vamos usar \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('floresta')   # conjunto de textos em português anotados com etiquetas morfossintáticas\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lamatizarPalavras(palavra):\n",
    "    stemmer = RSLPStemmer()     # Removedor de Sufixos da Língua Portuguesa\n",
    "    lemma = stemmer.stem(palavra)\n",
    "    if lemma == palavra:\n",
    "        synsets = wordnet.synsets(palavra, lang='por')\n",
    "        if synsets:\n",
    "            lemma = synsets[0].lemmas()[0].name()\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lamatizarTexto(texto):\n",
    "    palavras = texto.split()\n",
    "    stopwordsPT = stopwords.words('portuguese')\n",
    "    textoLematizado = [lamatizarPalavras(palavra.lower()) for palavra in palavras if palavra.lower() not in stopwordsPT]\n",
    "    textoLematizado = ' '.join(textoLematizado)\n",
    "    return textoLematizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo = PyPDF2.PdfReader('ArquivosPT/DAR20052019.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(artigo.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "textin = artigo.pages[7].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n \\n  \\nRESUMO   \\nControlar o tráfego rodoviário, seja em vias públicas ou privadas, as fronteiras, quem estaciona \\nem um determinado estacionamento, ou quem está infringindo as leis de trânsito são exemplos \\nde tarefas que exigem a identificação veicular, identificação essa feita por meio de placas de \\nlicenciament o automotivas reguladas por leis específicas. Para o processo de identificação de \\nplacas é necessário o uso de alguma técnica de Visão Computacional , campo este que tem \\nganhado bastante atenção da comunidade científica e das empresas ao longo dos últimos a nos \\ncomo forma de aumentar eficiência e cortar custos.  Com o advento da Aprendizagem Profunda, \\nsoluções para os mais diversos problemas vêm sendo pesquisadas.  Com as Redes Neurais \\nArtificiais, especialmente as Redes Neurais Convolucionais, um salto  foi dad o em termos de \\nresultados se comparados com técnicas tradicionai s. As Redes Neurais Convolucionais se \\nmostram extremamente bem -sucedidas em tarefas que envolvem principalmente classificação.  \\nEste trabalho busc ou demonstrar a viabilidade de uma Rede Neural Convolucional para leitura \\nde caracteres em placas de licenciamento veiculares , obtendo resultados de inferência na ordem \\nde 89,24% de caracteres inferidos corretamente . \\n \\nPalavras -chave:  aprendizagem profunda, redes neurais convolucionais, reconhecimento , \\nplacas de licenciamento veiculares  \\n  '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832\n"
     ]
    }
   ],
   "source": [
    "texto = lamatizarTexto(textin)\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resum control tráfeg rodoviário, via públic privadas, fronteiras, estac determin estacionamento, infring lei trânsit exempl taref exig identific veicular, identific feit mei plac licenciament automo regul lei específicas. process identific plac necess wear algum técn vis computac , camp ganh bast atenç comunidad científ empr long últ form aument efici cort custos. advent aprendiz profunda, soluç divers problem vêm send pesquisadas. red neur artificiais, espec red neur convolucionais, salt dad term result compar técn tradicion s. red neur convoluc mostr extrem well -suced taref envolv princip classificação. trabalh busc demonstr viabil red neur convoluc leit caract plac licenc veicul , obt result infer ord 89,24% caract infer corret . palavr -chave: aprendiz profunda, red neur convolucionais, reconhec , plac licenc veicul'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a7084dce4a57fabdb294815b03e7954449d6920124286f14e8400459bb21104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
