{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extração do texto do PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'al', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'et', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Listagem de palavras frequentes que não alteram o significado do texto (em ingles)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "irrelevantes = stopwords.words('english')\n",
    "irrelevantes.append('et')\n",
    "irrelevantes.append('al')\n",
    "irrelevantes.sort()\n",
    "print(irrelevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rafae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tratamento de flexão de palavras por lematização\n",
    "tk = nltk.tokenize.WhitespaceTokenizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lmt = nltk.stem.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A biblioteca nltk não tem dados para lematização em PT ;-;\n",
    "def lematize_text(text):\n",
    "    lem_words = list()\n",
    "    for word in tk.tokenize(text):\n",
    "        lem_words.append(lmt.lemmatize(word))\n",
    "    return lem_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfReader('ArquivosEN/1608.06902.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recurrent neural networks with limited numerical precision\n",
      "joachim ott\u0003, zhouhan linz,ying zhangz, shih-chii liu\u0003, yoshua bengiozy\n",
      "\u0003institute of neuroinformatics, university of zurich and eth zurich\n",
      "ottj@ethz.ch, shih@ini.ethz.ch\n",
      "zdépartement d’informatique et de recherche opérationnelle, université de montréal\n",
      "ycifar senior fellow\n",
      "{zhouhan.lin, ying.zhang}@umontreal.ca\n",
      "abstract\n",
      "recurrent neural networks (rnns) produce state-of-art performance on many machine learning\n",
      "tasks but their demand on resources in terms of memory and computational power are often\n",
      "high. therefore, there is a great interest in optimizing the computations performed with these\n",
      "models especially when considering development of specialized low-power hardware for deep\n",
      "networks. one way of reducing the computational needs is to limit the numerical precision of\n",
      "the network weights and biases. this has led to different proposed rounding methods which\n",
      "have been applied so far to only convolutional neural networks and fully-connected networks.\n",
      "this paper addresses the question of how to best reduce weight precision during training in the\n",
      "case of rnns. we present results from the use of different stochastic and deterministic reduced\n",
      "precision training methods applied to three major rnn types which are then tested on several\n",
      "datasets. the results show that the weight binarization methods do not work with the rnns.\n",
      "however, the stochastic and deterministic ternarization, and pow2-ternarization methods gave\n",
      "rise to low-precision rnns that produce similar and even higher accuracy on certain datasets\n",
      "therefore providing a path towards training more efﬁcient implementations of rnns in specialized\n",
      "hardware.\n",
      "1 introduction\n",
      "a recurrent neural network (rnn) is a speciﬁc type of neural network which is able to process input and\n",
      "output sequences of variable length. because of this nature, rnns are suitable for sequence modeling. various\n",
      "rnn architectures have been proposed in recent years, based on different forms of non-linearity, such as the\n",
      "gated recurrent unit (gru) [cho et al., 2014] and long-short term memory (lstm) [hochreiter et al.,\n",
      "1997]. they have enabled new levels of performance in many tasks such as speech recognition [amodei et al.,\n",
      "2015][chan et al., 2015], machine translation [devlin et al., 2014][chung et al., 2016][sutskever et al., 2014],\n",
      "or even video games [mnih et al., 2015] and go[silver et al., 2016].\n",
      "compared to standard feed-forward networks, rnns often take longer to train and are more demanding\n",
      "in memory and computational power. for example, it can take weeks to train models for state-of-the-art\n",
      "machine translation and speech recognition. thus it is of vital importance to accelerate computation and\n",
      "reduce training time of such networks. on the other hand, even at run-time, these models require too much in\n",
      "terms of computational resources if we want to deploy a model onto low-power embedded hardware devices.\n",
      "increasingly, dedicated deep learning hardware platforms including fpgas [farabet et al., 2011] and custom\n",
      "chips [sim et al., 2016] are reporting higher computational efﬁciencies of up to tera operations per second per\n",
      "watt (tops/w). these platforms are targeted at deep cnns. if low-precision rnns are able to report the\n",
      "same performance, then the savings in the reduction of multipliers (the circuits that take the space and energy)\n",
      "and memory storage of the weights would be even larger as the bit precision of the multipliers needed for the 2\n",
      "to 3 gates of the gated rnn units can be reduced or the multipliers removed completely.\n",
      "1arxiv:1608.06902v2  [cs.ne]  26 feb 2017\n"
     ]
    }
   ],
   "source": [
    "#Lendo uma pagina\n",
    "textin = reader.pages[0].extract_text()\n",
    "textin = textin.lower()\n",
    "print(textin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "['\\x03institute', '(gru)', '(lstm)', '(rnn)', '(rnns)', '(the', '(tops/w).', '1', '1997].', '1arxiv:1608.06902v2', '2', '2011]', '2014]', '2014],', '2014][chung', '2015]', '2015],', '2015][chan', '2016]', '2016].', '2016][sutskever', '2017', '26', '3', '[amodei', '[cho', '[cs.ne]', '[devlin', '[farabet', '[hochreiter', '[mnih', '[sim', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'able', 'able', 'abstract', 'accelerate', 'accuracy', 'address', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'al.,', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'applied', 'applied', 'architecture', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'at', 'at', 'based', 'be', 'be', 'because', 'been', 'been', 'bengiozy', 'best', 'biases.', 'binarization', 'bit', 'but', 'can', 'can', 'case', 'certain', 'chip', 'circuit', 'cnns.', 'compared', 'completely.', 'computation', 'computation', 'computational', 'computational', 'computational', 'computational', 'computational', 'considering', 'convolutional', 'custom', 'datasets', 'datasets.', 'de', 'de', 'dedicated', 'deep', 'deep', 'deep', 'demand', 'demanding', 'deploy', 'deterministic', 'deterministic', 'development', 'devices.', 'different', 'different', 'different', 'do', 'during', 'd’informatique', 'efﬁciencies', 'efﬁcient', 'embedded', 'enabled', 'energy)', 'especially', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'eth', 'even', 'even', 'even', 'even', 'example,', 'far', 'feb', 'feed-forward', 'fellow', 'for', 'for', 'for', 'for', 'for', 'form', 'fpgas', 'from', 'fully-connected', 'game', 'gate', 'gated', 'gated', 'gave', 'go[silver', 'great', 'ha', 'hand,', 'hardware', 'hardware', 'hardware', 'hardware.', 'have', 'have', 'have', 'high.', 'higher', 'higher', 'how', 'however,', 'if', 'if', 'implementation', 'importance', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'including', 'increasingly,', 'input', 'interest', 'introduction', 'is', 'is', 'is', 'is', 'is', 'it', 'it', 'joachim', 'larger', 'learning', 'learning', 'led', 'length.', 'level', 'limit', 'limited', 'linz,ying', 'liu\\x03,', 'long-short', 'longer', 'low-power', 'low-power', 'low-precision', 'low-precision', 'machine', 'machine', 'machine', 'major', 'many', 'many', 'memory', 'memory', 'memory', 'memory', 'method', 'method', 'method', 'method', 'model', 'model', 'model', 'model', 'modeling.', 'montréal', 'more', 'more', 'much', 'multiplier', 'multiplier', 'multiplier', 'nature,', 'need', 'needed', 'network', 'network', 'network', 'network', 'network', 'network', 'networks,', 'networks.', 'networks.', 'networks.', 'neural', 'neural', 'neural', 'neural', 'neural', 'neuroinformatics,', 'new', 'non-linearity,', 'not', 'numerical', 'numerical', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'often', 'often', 'on', 'on', 'on', 'on', 'on', 'on', 'one', 'only', 'onto', 'operation', 'optimizing', 'opérationnelle,', 'or', 'or', 'other', 'ott\\x03,', 'ottj@ethz.ch,', 'output', 'paper', 'path', 'per', 'per', 'performance', 'performance', 'performance,', 'performed', 'platform', 'platform', 'pow2-ternarization', 'power', 'power.', 'precision', 'precision', 'precision', 'precision', 'precision', 'present', 'process', 'produce', 'produce', 'proposed', 'proposed', 'providing', 'question', 'recent', 'recherche', 'recognition', 'recognition.', 'recurrent', 'recurrent', 'recurrent', 'recurrent', 'reduce', 'reduce', 'reduced', 'reduced', 'reducing', 'reduction', 'removed', 'report', 'reporting', 'require', 'resource', 'resource', 'result', 'result', 'rise', 'rnn', 'rnn', 'rnn', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns.', 'rnns.', 'rounding', 'run-time,', 'same', 'saving', 'second', 'senior', 'sequence', 'sequence', 'several', 'shih-chii', 'shih@ini.ethz.ch', 'show', 'similar', 'so', 'space', 'specialized', 'specialized', 'speciﬁc', 'speech', 'speech', 'standard', 'state-of-art', 'state-of-the-art', 'stochastic', 'stochastic', 'storage', 'such', 'such', 'such', 'suitable', 'take', 'take', 'take', 'targeted', 'task', 'task', 'tera', 'term', 'term', 'term', 'ternarization,', 'tested', 'that', 'that', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'their', 'then', 'then', 'there', 'therefore', 'therefore,', 'these', 'these', 'these', 'they', 'this', 'this', 'this', 'three', 'thus', 'time', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'too', 'towards', 'train', 'train', 'training', 'training', 'training', 'training', 'translation', 'translation', 'type', 'type', 'unit', 'unit', 'university', 'université', 'up', 'use', 'variable', 'various', 'video', 'vital', 'want', 'watt', 'way', 'we', 'we', 'week', 'weight', 'weight', 'weight', 'weight', 'when', 'which', 'which', 'which', 'with', 'with', 'with', 'work', 'would', 'ycifar', 'years,', 'ying.zhang}@umontreal.ca', 'yoshua', 'zdépartement', 'zhangz,', 'zhouhan', 'zurich', 'zurich', '{zhouhan.lin,']\n"
     ]
    }
   ],
   "source": [
    "# Tem que deixar as letras em minusculo ;-;\n",
    "lematizedTXT = lematize_text(textin)\n",
    "lematizedTXT.sort()\n",
    "print(len(lematizedTXT))\n",
    "print(lematizedTXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x03institute', 'gru', 'lstm', 'rnn', 'rnns', 'the', 'tops/w)', '1', '1997]', '1arxiv:1608.06902v2', '2', '2011', '2014', '2014]', '2014][chung', '2015', '2015]', '2015][chan', '2016', '2016]', '2016][sutskever', '2017', '26', '3', 'amodei', 'cho', 'cs.ne', 'devlin', 'farabet', 'hochreiter', 'mnih', 'sim', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'able', 'able', 'abstract', 'accelerate', 'accuracy', 'address', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'applied', 'applied', 'architecture', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'at', 'at', 'based', 'be', 'be', 'because', 'been', 'been', 'bengiozy', 'best', 'biases', 'binarization', 'bit', 'but', 'can', 'can', 'case', 'certain', 'chip', 'circuit', 'cnns', 'compared', 'completely', 'computation', 'computation', 'computational', 'computational', 'computational', 'computational', 'computational', 'considering', 'convolutional', 'custom', 'datasets', 'datasets', 'de', 'de', 'dedicated', 'deep', 'deep', 'deep', 'demand', 'demanding', 'deploy', 'deterministic', 'deterministic', 'development', 'devices', 'different', 'different', 'different', 'do', 'during', 'd’informatique', 'efﬁciencies', 'efﬁcient', 'embedded', 'enabled', 'energy', 'especially', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'eth', 'even', 'even', 'even', 'even', 'example', 'far', 'feb', 'feed-forward', 'fellow', 'for', 'for', 'for', 'for', 'for', 'form', 'fpgas', 'from', 'fully-connected', 'game', 'gate', 'gated', 'gated', 'gave', 'go[silver', 'great', 'ha', 'hand', 'hardware', 'hardware', 'hardware', 'hardware', 'have', 'have', 'have', 'high', 'higher', 'higher', 'how', 'however', 'if', 'if', 'implementation', 'importance', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'including', 'increasingly', 'input', 'interest', 'introduction', 'is', 'is', 'is', 'is', 'is', 'it', 'it', 'joachim', 'larger', 'learning', 'learning', 'led', 'length', 'level', 'limit', 'limited', 'linz,ying', 'liu\\x03', 'long-short', 'longer', 'low-power', 'low-power', 'low-precision', 'low-precision', 'machine', 'machine', 'machine', 'major', 'many', 'many', 'memory', 'memory', 'memory', 'memory', 'method', 'method', 'method', 'method', 'model', 'model', 'model', 'model', 'modeling', 'montréal', 'more', 'more', 'much', 'multiplier', 'multiplier', 'multiplier', 'nature', 'need', 'needed', 'network', 'network', 'network', 'network', 'network', 'network', 'networks', 'networks', 'networks', 'networks', 'neural', 'neural', 'neural', 'neural', 'neural', 'neuroinformatics', 'new', 'non-linearity', 'not', 'numerical', 'numerical', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'often', 'often', 'on', 'on', 'on', 'on', 'on', 'on', 'one', 'only', 'onto', 'operation', 'optimizing', 'opérationnelle', 'or', 'or', 'other', 'ott\\x03', 'ottj@ethz.ch', 'output', 'paper', 'path', 'per', 'per', 'performance', 'performance', 'performance', 'performed', 'platform', 'platform', 'pow2-ternarization', 'power', 'power', 'precision', 'precision', 'precision', 'precision', 'precision', 'present', 'process', 'produce', 'produce', 'proposed', 'proposed', 'providing', 'question', 'recent', 'recherche', 'recognition', 'recognition', 'recurrent', 'recurrent', 'recurrent', 'recurrent', 'reduce', 'reduce', 'reduced', 'reduced', 'reducing', 'reduction', 'removed', 'report', 'reporting', 'require', 'resource', 'resource', 'result', 'result', 'rise', 'rnn', 'rnn', 'rnn', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rounding', 'run-time', 'same', 'saving', 'second', 'senior', 'sequence', 'sequence', 'several', 'shih-chii', 'shih@ini.ethz.ch', 'show', 'similar', 'so', 'space', 'specialized', 'specialized', 'speciﬁc', 'speech', 'speech', 'standard', 'state-of-art', 'state-of-the-art', 'stochastic', 'stochastic', 'storage', 'such', 'such', 'such', 'suitable', 'take', 'take', 'take', 'targeted', 'task', 'task', 'tera', 'term', 'term', 'term', 'ternarization', 'tested', 'that', 'that', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'their', 'then', 'then', 'there', 'therefore', 'therefore', 'these', 'these', 'these', 'they', 'this', 'this', 'this', 'three', 'thus', 'time', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'too', 'towards', 'train', 'train', 'training', 'training', 'training', 'training', 'translation', 'translation', 'type', 'type', 'unit', 'unit', 'university', 'université', 'up', 'use', 'variable', 'various', 'video', 'vital', 'want', 'watt', 'way', 'we', 'we', 'week', 'weight', 'weight', 'weight', 'weight', 'when', 'which', 'which', 'which', 'with', 'with', 'with', 'work', 'would', 'ycifar', 'years', 'ying.zhang}@umontreal.ca', 'yoshua', 'zdépartement', 'zhangz', 'zhouhan', 'zurich', 'zurich', '{zhouhan.lin']\n"
     ]
    }
   ],
   "source": [
    "# Tirar o ponto do final do bagulho\n",
    "punctuation = [\".\", \",\", \"!\", \"?\", \";\", \":\", \"(\", \")\", \"[\", \"]\", \".,\"]\n",
    "for word in range(0, len(lematizedTXT)):\n",
    "    if lematizedTXT[word][-2:] in punctuation:\n",
    "        lematizedTXT[word] = lematizedTXT[word][:-2]\n",
    "    elif lematizedTXT[word][-1] in punctuation:\n",
    "        lematizedTXT[word] = lematizedTXT[word][:-1]\n",
    "    if lematizedTXT[word][0] in punctuation:\n",
    "        lematizedTXT[word] = lematizedTXT[word][1:]\n",
    "\n",
    "print(lematizedTXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'al', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'at', 'at', 'be', 'be', 'because', 'been', 'been', 'but', 'can', 'can', 'do', 'during', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'et', 'for', 'for', 'for', 'for', 'for', 'from', 'have', 'have', 'have', 'how', 'if', 'if', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'is', 'is', 'is', 'is', 'is', 'it', 'it', 'more', 'more', 'not', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'on', 'on', 'on', 'on', 'on', 'on', 'only', 'or', 'or', 'other', 'same', 'so', 'such', 'such', 'such', 'that', 'that', 'that', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'their', 'then', 'then', 'there', 'these', 'these', 'these', 'they', 'this', 'this', 'this', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'too', 'up', 'we', 'we', 'when', 'which', 'which', 'which', 'with', 'with', 'with']\n"
     ]
    }
   ],
   "source": [
    "#Removendo elementos ruins - função bem custosa, deve ter alguma que faz isso mais facil\n",
    "removeList = list()\n",
    "for word in lematizedTXT:\n",
    "    for stopword in irrelevantes:\n",
    "        if word == stopword:\n",
    "            removeList.append(word)\n",
    "for i in removeList:\n",
    "    lematizedTXT.remove(i)\n",
    "print(removeList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n",
      "['\\x03institute', '1', '1997]', '1arxiv:1608.06902v2', '2', '2011', '2014', '2014]', '2014][chung', '2015', '2015]', '2015][chan', '2016', '2016]', '2016][sutskever', '2017', '26', '3', 'able', 'able', 'abstract', 'accelerate', 'accuracy', 'address', 'amodei', 'applied', 'applied', 'architecture', 'based', 'bengiozy', 'best', 'biases', 'binarization', 'bit', 'case', 'certain', 'chip', 'cho', 'circuit', 'cnns', 'compared', 'completely', 'computation', 'computation', 'computational', 'computational', 'computational', 'computational', 'computational', 'considering', 'convolutional', 'cs.ne', 'custom', 'datasets', 'datasets', 'de', 'de', 'dedicated', 'deep', 'deep', 'deep', 'demand', 'demanding', 'deploy', 'deterministic', 'deterministic', 'development', 'devices', 'devlin', 'different', 'different', 'different', 'd’informatique', 'efﬁciencies', 'efﬁcient', 'embedded', 'enabled', 'energy', 'especially', 'eth', 'even', 'even', 'even', 'even', 'example', 'far', 'farabet', 'feb', 'feed-forward', 'fellow', 'form', 'fpgas', 'fully-connected', 'game', 'gate', 'gated', 'gated', 'gave', 'go[silver', 'great', 'gru', 'ha', 'hand', 'hardware', 'hardware', 'hardware', 'hardware', 'high', 'higher', 'higher', 'hochreiter', 'however', 'implementation', 'importance', 'including', 'increasingly', 'input', 'interest', 'introduction', 'joachim', 'larger', 'learning', 'learning', 'led', 'length', 'level', 'limit', 'limited', 'linz,ying', 'liu\\x03', 'long-short', 'longer', 'low-power', 'low-power', 'low-precision', 'low-precision', 'lstm', 'machine', 'machine', 'machine', 'major', 'many', 'many', 'memory', 'memory', 'memory', 'memory', 'method', 'method', 'method', 'method', 'mnih', 'model', 'model', 'model', 'model', 'modeling', 'montréal', 'much', 'multiplier', 'multiplier', 'multiplier', 'nature', 'need', 'needed', 'network', 'network', 'network', 'network', 'network', 'network', 'networks', 'networks', 'networks', 'networks', 'neural', 'neural', 'neural', 'neural', 'neural', 'neuroinformatics', 'new', 'non-linearity', 'numerical', 'numerical', 'often', 'often', 'one', 'onto', 'operation', 'optimizing', 'opérationnelle', 'ott\\x03', 'ottj@ethz.ch', 'output', 'paper', 'path', 'per', 'per', 'performance', 'performance', 'performance', 'performed', 'platform', 'platform', 'pow2-ternarization', 'power', 'power', 'precision', 'precision', 'precision', 'precision', 'precision', 'present', 'process', 'produce', 'produce', 'proposed', 'proposed', 'providing', 'question', 'recent', 'recherche', 'recognition', 'recognition', 'recurrent', 'recurrent', 'recurrent', 'recurrent', 'reduce', 'reduce', 'reduced', 'reduced', 'reducing', 'reduction', 'removed', 'report', 'reporting', 'require', 'resource', 'resource', 'result', 'result', 'rise', 'rnn', 'rnn', 'rnn', 'rnn', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rnns', 'rounding', 'run-time', 'saving', 'second', 'senior', 'sequence', 'sequence', 'several', 'shih-chii', 'shih@ini.ethz.ch', 'show', 'sim', 'similar', 'space', 'specialized', 'specialized', 'speciﬁc', 'speech', 'speech', 'standard', 'state-of-art', 'state-of-the-art', 'stochastic', 'stochastic', 'storage', 'suitable', 'take', 'take', 'take', 'targeted', 'task', 'task', 'tera', 'term', 'term', 'term', 'ternarization', 'tested', 'therefore', 'therefore', 'three', 'thus', 'time', 'tops/w)', 'towards', 'train', 'train', 'training', 'training', 'training', 'training', 'translation', 'translation', 'type', 'type', 'unit', 'unit', 'university', 'université', 'use', 'variable', 'various', 'video', 'vital', 'want', 'watt', 'way', 'week', 'weight', 'weight', 'weight', 'weight', 'work', 'would', 'ycifar', 'years', 'ying.zhang}@umontreal.ca', 'yoshua', 'zdépartement', 'zhangz', 'zhouhan', 'zurich', 'zurich', '{zhouhan.lin']\n"
     ]
    }
   ],
   "source": [
    "#Deu certo, mas ainda tem palavras que precisariam ser tratadas\n",
    "print(len(lematizedTXT))\n",
    "lematizedTXT.sort()\n",
    "print(lematizedTXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existem caracteres estranhos (Ott\\x03,)\n",
    "#Verificar pontuação, ela pode não ser tão importante\n",
    "#Existem alguns trechos em espanhol(?)\n",
    "#Palavras compostas como low-precision\n",
    "#1arXiv:1608.06902v2 (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 6, 5, 5, 5, 4, 4, 4, 4, 4]\n",
      "['rnns', 'network', 'computational', 'neural', 'precision', 'even', 'hardware', 'memory', 'method', 'model']\n"
     ]
    }
   ],
   "source": [
    "#Função deu certo, mas tá feia!!!\n",
    "#Lista com os termos mais comuns e com o numero de ocorrencia\n",
    "commonTerms = [None, None, None, None, None, None, None, None, None, None]\n",
    "bestTimes = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for word in range(0, len(lematizedTXT)):\n",
    "    times = lematizedTXT.count(lematizedTXT[word])\n",
    "    for i in range(0,10):\n",
    "        if lematizedTXT[word] == lematizedTXT[word-1]:\n",
    "            pass\n",
    "        elif bestTimes[i] < times:\n",
    "            bestTimes.insert(i, times)\n",
    "            commonTerms.insert(i, lematizedTXT[word])\n",
    "            bestTimes.pop()\n",
    "            commonTerms.pop()\n",
    "            break\n",
    "print(bestTimes)\n",
    "print(commonTerms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a7084dce4a57fabdb294815b03e7954449d6920124286f14e8400459bb21104"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
